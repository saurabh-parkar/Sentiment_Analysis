{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_Sentiment_Analysis_using_Torchtext_and_BiLSTM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMTR7RM2LPVc3SDru2T5V5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabh-parkar/Sentiment_Analysis/blob/master/IMDB_Sentiment_Analysis_using_Torchtext_and_BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sJPp203cxch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCT_hCr4c09f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(lower=True, batch_first=True, include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61nTKrrvc1T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-okyh89c2bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gGOV_c3peiM",
        "colab_type": "text"
      },
      "source": [
        "###**build_vocab**\n",
        "\n",
        "build_vocab will create the Vocab object for Field, which contains the information to convert word into word index and vice versa.\n",
        "\n",
        "The build_vocab also helps to download the word embeddings and associate them with the words in the vocabulary. The word embedding will save as Field.Vocab.vectors. vectors contains all of the word embedding. These be used later using embed.weight.data.copy_(torch.from_numpy(pretrained_weight)) when we define the nn.embedding layer.\n",
        "\n",
        "By default torchtext will initialize the unknown word vectors not in the vocabulary (pretrained embeddings) to zero, we initialize them with a random Gaussian distribution using unk_init argument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAwAV6vldWAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOGtpaUfpkq2",
        "colab_type": "text"
      },
      "source": [
        "### **BucketIterator**\n",
        "\n",
        "**data.BucketIterator.splits** returns iterators that loads batches of data from datasets each having a text attribute and a label attribute, and the text in same batch will have similar lengths. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGhVo3FdWLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPlcvwQpr0A",
        "colab_type": "text"
      },
      "source": [
        "### **Model**\n",
        "\n",
        "If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your LSTM will give output of shape (seq_len, batch_size, hidden_size).\n",
        "\n",
        "If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your LSTM will give output of shape (batch_size, seq_len, hidden_size).\n",
        "\n",
        "The -1 in .view means that it infer the actual value for this dimension based on the other values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWHknTHjemNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    # define all the layers used in model\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, output_dim, n_layers,\n",
        "                bidirectional, dropout, pad_index):\n",
        "    # Constructor\n",
        "    super().__init__()\n",
        "\n",
        "    # embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
        "\n",
        "    # lstm layer\n",
        "    self.lstm = nn.LSTM(embedding_dim,\n",
        "                        hidden_dim1,\n",
        "                        num_layers=n_layers,\n",
        "                        bidirectional= bidirectional,\n",
        "                        batch_first=True,\n",
        "                        dropout = dropout)\n",
        "    \n",
        "    self.fc1 = nn.Linear(hidden_dim1 * 2, output_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim2, output_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # activation function\n",
        "    self.act = nn.Softmax() #\\ F.log_softmax(outp)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "\n",
        "    # text = [batch size,sent_length]\n",
        "    embedded = self.embedding(text)\n",
        "    # embedded = [batch size, sent_len, emb dim]\n",
        "\n",
        "    # packed sequence\n",
        "    packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True) # unpad\n",
        "\n",
        "    packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "    # packed_output shape = (batch, seq_len, num_directions * hidden_size)\n",
        "    # hidden shape  = (num_layers * num_directions, batch, hidden_size)\n",
        "   \n",
        "    # batch_first doesnt affect the hidden and the cell states\n",
        "\n",
        "    # concat the final forward and backward hidden state\n",
        "    cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "    # output, output_lengths = pad_packed_sequence(packed_output)  # pad the sequence to the max length in the batch\n",
        "\n",
        "    res = self.dropout(cat)\n",
        "    preds = self.fc1(res)\n",
        "\n",
        "    # Final activation function\n",
        "    # preds = self.act(preds)\n",
        "    # preds = preds.argmax(dim=1).unsqueeze(0)\n",
        "    return preds"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Epx5k8Weuve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "620f04be-5714-44f5-b1af-5c530c6f3e14"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM1 = 256\n",
        "HIDDEN_DIM2 = 100\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM1, HIDDEN_DIM2, OUTPUT_DIM, N_LAYERS,\n",
        "                BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "model"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (act): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gREHhplRfHyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed906707-6bb3-4972-d0de-7eaf26107b06"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model)} trainable paramters')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4810958 trainable paramters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeX0vCGbfNZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d70a1ba8-345d-45d9-bacd-49b8101e205c"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acUcQrNnfNyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "119ee1dc-26e9-40ec-b71b-64bb940bfa4d"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.3114, -0.7845,  0.5564,  ...,  0.5830, -0.9701, -0.8863],\n",
              "        [-0.3194,  0.0241, -0.0103,  ...,  0.3187,  0.3611, -0.0141],\n",
              "        [-0.0060,  0.0117, -0.2508,  ...,  0.3381,  0.7595, -0.3132]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CepDzmLPfRCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "70af2346-ec84-4a9d-d547-4fe610156969"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.3114, -0.7845,  0.5564,  ...,  0.5830, -0.9701, -0.8863],\n",
            "        [-0.3194,  0.0241, -0.0103,  ...,  0.3187,  0.3611, -0.0141],\n",
            "        [-0.0060,  0.0117, -0.2508,  ...,  0.3381,  0.7595, -0.3132]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0inwrmnfRPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "527Mh0tUfShN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngdIH9hzfT5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def accuracy(preds, y):\n",
        "    predicted = torch.round(torch.sigmoid(preds))\n",
        "    correct = (predicted == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EMjJkWSfWcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        # retrieve text and no. of words\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label.squeeze())\n",
        "\n",
        "        acc = accuracy(predictions, batch.label)\n",
        "\n",
        "        # perform backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ILiiTR-fX_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label.squeeze())\n",
        "\n",
        "            acc = accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5grBEuofZns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILD_e7gfpxcA",
        "colab_type": "text"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7QceD55fa-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 6\n",
        "\n",
        "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "      \n",
        "        # train the model\n",
        "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "\n",
        "        # evaluate the model\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "\n",
        "        # save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2kBLw2LyK-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "77d294cd-2013-4d57-ce23-cdb39aca8cdd"
      },
      "source": [
        "run_train(N_EPOCHS, model, train_iterator, valid_iterator, optimizer, criterion, \"BiLSTM\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 34s\n",
            "\tTrain Loss: 0.173 | Train Acc: 93.38%\n",
            "\t Val. Loss: 0.333 |  Val. Acc: 86.87%\n",
            "Epoch: 02 | Epoch Time: 0m 34s\n",
            "\tTrain Loss: 0.120 | Train Acc: 95.75%\n",
            "\t Val. Loss: 0.378 |  Val. Acc: 86.78%\n",
            "Epoch: 03 | Epoch Time: 0m 34s\n",
            "\tTrain Loss: 0.078 | Train Acc: 97.37%\n",
            "\t Val. Loss: 0.434 |  Val. Acc: 86.67%\n",
            "Epoch: 04 | Epoch Time: 0m 34s\n",
            "\tTrain Loss: 0.048 | Train Acc: 98.57%\n",
            "\t Val. Loss: 0.466 |  Val. Acc: 86.45%\n",
            "Epoch: 05 | Epoch Time: 0m 34s\n",
            "\tTrain Loss: 0.029 | Train Acc: 99.25%\n",
            "\t Val. Loss: 0.551 |  Val. Acc: 87.22%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIo2-BFfpJ4y",
        "colab_type": "text"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRIRLVNtoyCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa59fdba-f41e-4dd9-c872-955b6a24bab6"
      },
      "source": [
        "# Testing\n",
        "\n",
        "model.load_state_dict(torch.load('/content/saved_weights_BiLSTM.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.356 | Test Acc: 85.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9PxAnPwpNvD",
        "colab_type": "text"
      },
      "source": [
        "### **Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsJB5PMxfcxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss_and_accuracy(history):\n",
        "    fig, axs = plt.subplots(1, 2, sharex=True)\n",
        "\n",
        "    axs[0].plot(history.history['loss'])\n",
        "    axs[0].plot(history.history['val_loss'])\n",
        "    axs[0].set_title('Model Loss')\n",
        "    axs[0].legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    axs[1].plot(history.history['acc'])\n",
        "    axs[1].plot(history.history['val_acc'])\n",
        "    axs[1].set_title('Model Accuracy')\n",
        "    axs[1].legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dm2S6HZn13B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}